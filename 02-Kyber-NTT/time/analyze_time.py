#!/usr/bin/env python3
"""
Analyze NTT timing log files generated by run_ntt_time.sh

Log format:
# Comments...
test_num trial sample time(sec)

Supports both simple analysis and success rate analysis by trial.
"""

import argparse
import sys
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

# Try to import scientific libraries
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    from scipy import stats
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


def load_log_file(filepath: Path) -> Dict[int, Dict[int, List[float]]]:
    """Load timing data from log file organized by test_num and trial.

    Returns:
        Dict mapping test_num -> trial -> list of timing values
        Example: {0: {1: [0.001, 0.002, ...], 2: [...]}, 1: {...}}
    """
    data = defaultdict(lambda: defaultdict(list))

    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            parts = line.split()
            # Support both old format (4 cols) and new format (5 cols with round)
            if len(parts) == 4:
                # Old format: test_num trial sample time
                test_num = int(parts[0])
                trial = int(parts[1])
                # sample = int(parts[2])  # Not needed
                time_val = float(parts[3])
                data[test_num][trial].append(time_val)
            elif len(parts) == 5:
                # New format: test_num trial round sample time
                test_num = int(parts[0])
                trial = int(parts[1])
                # round = int(parts[2])  # Not needed for per-trial analysis
                # sample = int(parts[3])  # Not needed
                time_val = float(parts[4])
                data[test_num][trial].append(time_val)

    return dict(data)


def compute_stats(times: List[float]) -> Dict:
    """Compute statistics for timing data."""
    if HAS_NUMPY:
        times_arr = np.array(times)
        return {
            'count': len(times),
            'mean': np.mean(times_arr),
            'std': np.std(times_arr),
            'min': np.min(times_arr),
            'max': np.max(times_arr),
            'median': np.median(times_arr),
            'p25': np.percentile(times_arr, 25),
            'p75': np.percentile(times_arr, 75),
        }
    else:
        times_sorted = sorted(times)
        n = len(times)
        mean_val = sum(times) / n
        return {
            'count': n,
            'mean': mean_val,
            'std': (sum((x - mean_val)**2 for x in times) / n) ** 0.5,
            'min': times_sorted[0],
            'max': times_sorted[-1],
            'median': times_sorted[n//2],
            'p25': times_sorted[n//4] if n >= 4 else times_sorted[0],
            'p75': times_sorted[3*n//4] if n >= 4 else times_sorted[-1],
        }


def analyze_success_rate(data: Dict[int, Dict[int, List[float]]]) -> Tuple[int, int, List[Dict]]:
    """Analyze success rate by comparing trials.

    For each trial, compare mean([C,C]) vs mean([C,0]).
    Success: [C,0] < [C,C] (C,0 should be faster)

    Returns:
        (success_count, total_trials, trial_details)
    """
    if 0 not in data or 1 not in data:
        return 0, 0, []

    trials_cc = data[0]  # test_num=0: [C,C]
    trials_c0 = data[1]  # test_num=1: [C,0]

    # Find common trials
    common_trials = set(trials_cc.keys()) & set(trials_c0.keys())

    if not common_trials:
        return 0, 0, []

    success_count = 0
    trial_details = []

    for trial in sorted(common_trials):
        times_cc = trials_cc[trial]
        times_c0 = trials_c0[trial]

        if HAS_NUMPY:
            mean_cc = np.mean(times_cc)
            mean_c0 = np.mean(times_c0)
            std_cc = np.std(times_cc)
            std_c0 = np.std(times_c0)
        else:
            mean_cc = sum(times_cc) / len(times_cc)
            mean_c0 = sum(times_c0) / len(times_c0)
            std_cc = (sum((x - mean_cc)**2 for x in times_cc) / len(times_cc)) ** 0.5
            std_c0 = (sum((x - mean_c0)**2 for x in times_c0) / len(times_c0)) ** 0.5

        diff = mean_cc - mean_c0  # diff = [C,C] - [C,0]
        diff_percent = (diff / mean_cc * 100) if mean_cc > 0 else 0
        is_success = mean_c0 < mean_cc  # Success: [C,0] faster than [C,C]

        if is_success:
            success_count += 1

        trial_details.append({
            'trial': trial,
            'mean_cc': mean_cc,
            'mean_c0': mean_c0,
            'std_cc': std_cc,
            'std_c0': std_c0,
            'diff': diff,  # Positive diff means [C,C] is slower (success)
            'diff_percent': diff_percent,
            'success': is_success,
            'samples_cc': len(times_cc),
            'samples_c0': len(times_c0),
            'times_cc': times_cc,  # Keep raw data for plotting
            'times_c0': times_c0,
        })

    # Sort by difference (largest positive first = C0 faster than CC by most)
    trial_details.sort(key=lambda x: x['diff'], reverse=True)

    return success_count, len(common_trials), trial_details


def print_analysis(data: Dict[int, Dict[int, List[float]]], show_trials: bool = False):
    """Print statistical analysis."""
    test_names = {0: "[C,C]", 1: "[C,0]"}

    print("=" * 80)
    print("NTT TIMING ANALYSIS")
    print("=" * 80)

    # Flatten data for overall statistics
    all_times = {}
    stats_by_test = {}

    for test_num in sorted(data.keys()):
        # Combine all trials for this test_num
        times = []
        for trial_times in data[test_num].values():
            times.extend(trial_times)

        all_times[test_num] = times
        stats_dict = compute_stats(times)
        stats_by_test[test_num] = stats_dict

        pattern_name = test_names.get(test_num, f"Test {test_num}")

        print(f"\n{pattern_name} Pattern (Overall):")
        print(f"  Trials:     {len(data[test_num])}")
        print(f"  Samples:    {stats_dict['count']}")
        print(f"  Mean:       {stats_dict['mean']:.9f} sec ({stats_dict['mean']*1000:.6f} ms)")
        print(f"  Std Dev:    {stats_dict['std']:.9f} sec ({stats_dict['std']*1000:.6f} ms)")
        print(f"  Min:        {stats_dict['min']:.9f} sec ({stats_dict['min']*1000:.6f} ms)")
        print(f"  Max:        {stats_dict['max']:.9f} sec ({stats_dict['max']*1000:.6f} ms)")
        print(f"  Median:     {stats_dict['median']:.9f} sec ({stats_dict['median']*1000:.6f} ms)")

    # Success rate analysis
    if 0 in stats_by_test and 1 in stats_by_test:
        print("\n" + "=" * 80)
        print("COMPARISON: [C,C] vs [C,0]")
        print("=" * 80)

        mean_cc = stats_by_test[0]['mean']
        mean_c0 = stats_by_test[1]['mean']
        diff = mean_cc - mean_c0  # Changed: [C,C] - [C,0]
        ratio = mean_cc / mean_c0 if mean_c0 > 0 else 0

        print(f"\nOverall Statistics:")
        print(f"  Mean [C,C]: {mean_cc:.9f} sec ({mean_cc*1000:.6f} ms)")
        print(f"  Mean [C,0]: {mean_c0:.9f} sec ({mean_c0*1000:.6f} ms)")
        print(f"  Difference (CC-C0): {diff:.9f} sec ({diff*1000:.6f} ms)")
        print(f"  Ratio (CC/C0):      {ratio:.6f}x")

        if diff > 0:
            print(f"  → [C,0] is FASTER by {abs(ratio-1)*100:.2f}% ✓ (Expected)")
        else:
            print(f"  → [C,C] is FASTER by {abs(ratio-1)*100:.2f}% ✗ (Unexpected)")

        # Statistical test
        if HAS_SCIPY:
            times_cc = all_times[0]
            times_c0 = all_times[1]
            t_stat, p_value = stats.ttest_ind(times_cc, times_c0)

            print(f"\n  t-test:")
            print(f"    t-statistic: {t_stat:.6f}")
            print(f"    p-value:     {p_value:.6f}")

            if p_value < 0.001:
                print("    → Highly significant difference (p < 0.001)")
            elif p_value < 0.01:
                print("    → Very significant difference (p < 0.01)")
            elif p_value < 0.05:
                print("    → Significant difference (p < 0.05)")
            else:
                print("    → No significant difference (p >= 0.05)")

            # Cohen's d
            std_cc = stats_by_test[0]['std']
            std_c0 = stats_by_test[1]['std']
            pooled_std = ((std_cc**2 + std_c0**2) / 2) ** 0.5
            cohens_d = (mean_cc - mean_c0) / pooled_std if pooled_std > 0 else 0

            print(f"\n  Cohen's d:   {cohens_d:.6f}")
            if abs(cohens_d) >= 0.8:
                print("    → Large effect size")
            elif abs(cohens_d) >= 0.5:
                print("    → Medium effect size")
            elif abs(cohens_d) >= 0.2:
                print("    → Small effect size")
            else:
                print("    → Negligible effect size")

        # Success rate by trial
        success_count, total_trials, trial_details = analyze_success_rate(data)

        if total_trials > 0:
            success_rate = success_count / total_trials

            print("\n" + "=" * 80)
            print("SUCCESS RATE ANALYSIS (By Trial)")
            print("=" * 80)
            print(f"\nSuccess criterion: mean([C,0]) < mean([C,C])")
            print(f"Success count: {success_count}/{total_trials}")
            print(f"Success rate:  {success_rate:.1%}")

            if trial_details:
                print("\nPer-Trial Details (sorted by C0 faster than CC, largest first):")
                print(f"{'Trial':<8} {'Mean_CC(ms)':<13} {'Mean_C0(ms)':<13} {'Diff(ms)':<12} {'Diff%':<10} {'Success':<8}")
                print("-" * 75)

                max_diff_trial = trial_details[0] if trial_details else None

                for i, detail in enumerate(trial_details):
                    success_mark = '✓' if detail['success'] else '✗'
                    is_max = (i == 0)  # First one is max after sorting
                    marker = " ★ BEST" if is_max else ""

                    print(f"{detail['trial']:<8} "
                          f"{detail['mean_cc']*1000:<13.6f} "
                          f"{detail['mean_c0']*1000:<13.6f} "
                          f"{detail['diff']*1000:<12.6f} "
                          f"{detail['diff_percent']:<10.3f} "
                          f"{success_mark:<8}"
                          f"{marker}")

                if max_diff_trial:
                    print(f"\n→ Trial {max_diff_trial['trial']} has C0 FASTER than CC by MOST: "
                          f"{max_diff_trial['diff']*1000:.6f}ms ({max_diff_trial['diff_percent']:.3f}%)")

    print("\n" + "=" * 80)


def plot_distributions(data: Dict[int, Dict[int, List[float]]], output_path: Path, trial_details: List[Dict] = None):
    """Plot timing distributions for the trial with largest difference."""
    if not HAS_MATPLOTLIB or not HAS_NUMPY:
        print("Warning: matplotlib or numpy not available, skipping plot")
        return

    test_names = {0: "[C,C]", 1: "[C,0]"}
    colors = {0: 'blue', 1: 'orange'}

    # Find the trial with maximum difference (C0 faster than CC by most)
    if trial_details and len(trial_details) > 0:
        max_diff_trial = trial_details[0]  # Already sorted by diff (largest positive first)
        trial_id = max_diff_trial['trial']

        print(f"\n→ Plotting Trial {trial_id} (C0 faster than CC by most: {max_diff_trial['diff']*1000:.6f}ms)")

        # Use data from the max diff trial only
        all_times = {
            0: np.array(max_diff_trial['times_cc']) * 1000,  # [C,C] in ms
            1: np.array(max_diff_trial['times_c0']) * 1000,  # [C,0] in ms
        }
    else:
        # Fallback: use all data (old behavior)
        all_times = {}
        for test_num in sorted(data.keys()):
            times = []
            for trial_times in data[test_num].values():
                times.extend(trial_times)
            all_times[test_num] = np.array(times) * 1000  # Convert to ms
        trial_id = "All"

    # Create single figure
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))

    # Compute percentile-based x-axis range (keep 95% of data, remove tails)
    all_data = np.concatenate([all_times[k] for k in all_times.keys()])
    p2_5, p97_5 = np.percentile(all_data, [2.5, 97.5])
    range_margin = (p97_5 - p2_5) * 0.05
    xlim_min = max(0, p2_5 - range_margin)
    xlim_max = p97_5 + range_margin

    # Plot for each test pattern
    for test_num in sorted(all_times.keys()):
        times = all_times[test_num]
        label = test_names.get(test_num, f"Test {test_num}")
        color = colors.get(test_num, 'gray')

        # Filter data to x-axis range
        times_filtered = times[(times >= xlim_min) & (times <= xlim_max)]
        mean_val = np.mean(times_filtered)
        median_val = np.median(times_filtered)

        # Plot histogram
        counts, bins, patches = ax.hist(times_filtered, bins=50, alpha=0.4,
                                        color=color, density=True, edgecolor=color, linewidth=0.8)

        # Plot smooth KDE curve using numpy-based Gaussian kernel
        # Simple KDE implementation without scipy
        x_range = np.linspace(xlim_min, xlim_max, 500)
        # Bandwidth using Scott's rule: n^(-1/5) * std
        bandwidth = len(times_filtered) ** (-1/5) * np.std(times_filtered)
        kde_values = np.zeros_like(x_range)
        for t in times_filtered:
            kde_values += np.exp(-0.5 * ((x_range - t) / bandwidth) ** 2)
        kde_values /= (len(times_filtered) * bandwidth * np.sqrt(2 * np.pi))

        ax.plot(x_range, kde_values, color=color, linewidth=3,
               label=f"{label}", alpha=0.9)

        # Mark mean
        ax.axvline(mean_val, color=color, linestyle='--', linewidth=2, alpha=0.6)

    # Add text annotations for means after all plots (so we can get proper y limits)
    for test_num in sorted(all_times.keys()):
        times = all_times[test_num]
        label = test_names.get(test_num, f"Test {test_num}")
        color = colors.get(test_num, 'gray')
        times_filtered = times[(times >= xlim_min) & (times <= xlim_max)]
        mean_val = np.mean(times_filtered)

        y_pos = ax.get_ylim()[1] * (0.90 if test_num == 0 else 0.80)
        ax.text(mean_val, y_pos, f'{label}\nmean={mean_val:.3f}ms',
               color=color, fontsize=10, ha='center', fontweight='bold',
               bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor=color, alpha=0.8, linewidth=1.5))

    ax.set_xlabel('Time (ms)', fontsize=12)
    ax.set_ylabel('Density', fontsize=12)

    # Set title with trial info
    if trial_details and len(trial_details) > 0:
        max_trial = trial_details[0]
        title = f'Trial {trial_id}: Best Case (C0 faster by most)\nDiff={max_trial["diff"]*1000:.6f}ms ({max_trial["diff_percent"]:.3f}%)'
    else:
        title = 'Timing Distribution (All Trials, 95% of data)'

    ax.set_title(title, fontsize=13, fontweight='bold')
    ax.set_xlim(xlim_min, xlim_max)
    ax.legend(fontsize=10, loc='upper right')
    ax.grid(True, alpha=0.3, linestyle='--')

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to: {output_path}")

    # Print statistics
    print(f"\nData range (full): {all_data.min():.3f}ms - {all_data.max():.3f}ms")
    print(f"P2.5-P97.5 range: {p2_5:.3f}ms - {p97_5:.3f}ms")
    print(f"Plot shows: {xlim_min:.3f}ms - {xlim_max:.3f}ms (95% of data)")
    outliers_pct = 100 * np.sum((all_data < p2_5) | (all_data > p97_5)) / len(all_data)
    print(f"Outliers outside range: {outliers_pct:.1f}%")

    # Print percentile comparison
    print(f"\nPercentile comparison:")
    for p in [50, 75, 90, 95, 97.5]:
        vals = {test_num: np.percentile(all_times[test_num], p)
                for test_num in sorted(all_times.keys())}
        if p == int(p):
            print(f"  P{int(p):2d}:   [C,C]={vals[0]:.3f}ms, [C,0]={vals[1]:.3f}ms, diff={vals[0]-vals[1]:.3f}ms")
        else:
            print(f"  P{p:.1f}: [C,C]={vals[0]:.3f}ms, [C,0]={vals[1]:.3f}ms, diff={vals[0]-vals[1]:.3f}ms")


def main():
    parser = argparse.ArgumentParser(description="Analyze NTT timing log files")
    parser.add_argument('logfile', type=Path, help='Path to log file')
    parser.add_argument('--plot', type=Path, help='Output path for plot (optional)')
    parser.add_argument('--show-trials', action='store_true',
                       help='Show per-trial details in success rate analysis')

    args = parser.parse_args()

    if not args.logfile.exists():
        print(f"Error: Log file not found: {args.logfile}", file=sys.stderr)
        sys.exit(1)

    print(f"Loading data from: {args.logfile}")
    data = load_log_file(args.logfile)

    if not data:
        print("Error: No timing data found in log file", file=sys.stderr)
        sys.exit(1)

    print_analysis(data, show_trials=args.show_trials)

    # Get trial details for plotting
    _, _, trial_details = analyze_success_rate(data)

    if args.plot:
        plot_distributions(data, args.plot, trial_details)
    elif HAS_MATPLOTLIB:
        # Auto-generate plot path
        plot_path = args.logfile.with_suffix('.png')
        plot_distributions(data, plot_path, trial_details)


if __name__ == '__main__':
    main()
